{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial of getting information from backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define some functions\n",
    "some backward has attributes `_saved*`, which save the most important information of calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def print_grad_fn(out):\n",
    "    grad = out.grad_fn\n",
    "    print(grad)\n",
    "    print([attr for attr in dir(grad) if attr[:6]==\"_saved\"])\n",
    "    print([sub_g for sub_g in grad.next_functions])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== convolution 1d start ==========\n",
      "<ConvolutionBackward0 object at 0x106d20be0>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23df0>, 0), (<AccumulateGrad object at 0x106d238b0>, 0)]\n",
      "========== convolution 1d end ==========\n",
      "========== convolution 2d start ==========\n",
      "<ConvolutionBackward0 object at 0x106d23d90>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23df0>, 0), (<AccumulateGrad object at 0x106d238b0>, 0)]\n",
      "========== convolution 2d end ==========\n",
      "========== convolution 3d start ==========\n",
      "<ConvolutionBackward0 object at 0x106d20be0>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23df0>, 0), (<AccumulateGrad object at 0x106d238b0>, 0)]\n",
      "========== convolution 3d end ==========\n",
      "<ConvolutionBackward0 object at 0x106d20be0>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23df0>, 0), (<AccumulateGrad object at 0x106d238b0>, 0)]\n",
      "========== convolution with groups end ==========\n",
      "========== convolution with dilation start ==========\n",
      "<ConvolutionBackward0 object at 0x106d23d90>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23df0>, 0), (<AccumulateGrad object at 0x106d238b0>, 0)]\n",
      "========== convolution with dilation end ==========\n"
     ]
    }
   ],
   "source": [
    "# convlution 1d\n",
    "print(\"=\"*10, \"convolution 1d start\", \"=\"*10)\n",
    "conv = nn.Conv1d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution 1d end\", \"=\"*10)\n",
    "# convolution 2d\n",
    "print(\"=\"*10, \"convolution 2d start\", \"=\"*10)\n",
    "conv = nn.Conv2d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution 2d end\", \"=\"*10)\n",
    "# convolution 3d\n",
    "print(\"=\"*10, \"convolution 3d start\", \"=\"*10)\n",
    "conv = nn.Conv3d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution 3d end\", \"=\"*10)\n",
    "# convolution with groups\n",
    "conv = nn.Conv2d(3, 3, 3, 1, 1, groups=3)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution with groups end\", \"=\"*10)\n",
    "# convolution with dilation\n",
    "print(\"=\"*10, \"convolution with dilation start\", \"=\"*10)\n",
    "conv = nn.Conv2d(3, 4, 3, 1, 1, dilation=2)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution with dilation end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Linear start ==========\n",
      "<AddmmBackward0 object at 0x106d20be0>\n",
      "['_saved_alpha', '_saved_beta', '_saved_mat1', '_saved_mat1_sym_sizes', '_saved_mat1_sym_strides', '_saved_mat2', '_saved_mat2_sym_sizes', '_saved_mat2_sym_strides']\n",
      "[(<AccumulateGrad object at 0x141cc5f90>, 0), (<AccumulateGrad object at 0x141cc5ff0>, 0), (<TBackward0 object at 0x141cc5990>, 0)]\n",
      "========== Linear end ==========\n",
      "========== Linear with input more than 2 dims start ==========\n",
      "<AddBackward0 object at 0x106d20be0>\n",
      "['_saved_alpha']\n",
      "[(<UnsafeViewBackward0 object at 0x141cc5f90>, 0), (<AccumulateGrad object at 0x141cc5ff0>, 0)]\n",
      "========== Linear with input more than 2 dims end ==========\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "print(\"=\"*10, \"Linear start\", \"=\"*10)\n",
    "fc = nn.Linear(3, 4)\n",
    "out = fc(torch.randn(1, 3, requires_grad=True))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"Linear end\", \"=\"*10)\n",
    "# Linear with input more than 2 dims\n",
    "print(\"=\"*10, \"Linear with input more than 2 dims start\", \"=\"*10)\n",
    "out = fc(torch.randn(1, 4, 4, 3, requires_grad=True))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"Linear with input more than 2 dims end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== batch norm start ==========\n",
      "<NativeBatchNormBackward0 object at 0x106d23700>\n",
      "['_saved_eps', '_saved_input', '_saved_result1', '_saved_result2', '_saved_running_mean', '_saved_running_var', '_saved_training', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d238b0>, 0), (<AccumulateGrad object at 0x106d23df0>, 0)]\n",
      "========== batch norm end ==========\n",
      "========== layer norm start ==========\n",
      "<NativeLayerNormBackward0 object at 0x106d23550>\n",
      "['_saved_bias', '_saved_input', '_saved_normalized_shape', '_saved_result1', '_saved_result2', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d238b0>, 0), (<AccumulateGrad object at 0x106d23df0>, 0)]\n",
      "========== layer norm end ==========\n",
      "========== group norm start ==========\n",
      "<NativeGroupNormBackward0 object at 0x106d23700>\n",
      "['_saved_C', '_saved_HxW', '_saved_N', '_saved_eps', '_saved_group', '_saved_input', '_saved_result1', '_saved_result2', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23550>, 0), (<AccumulateGrad object at 0x106d23df0>, 0)]\n",
      "========== group norm end ==========\n"
     ]
    }
   ],
   "source": [
    "# batch norm\n",
    "print(\"=\"*10, \"batch norm start\", \"=\"*10)\n",
    "bn = nn.BatchNorm2d(3)\n",
    "out = bn(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"batch norm end\", \"=\"*10)\n",
    "# # layer norm\n",
    "print(\"=\"*10, \"layer norm start\", \"=\"*10)\n",
    "ln = nn.LayerNorm([3, 4, 4])\n",
    "out = ln(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"layer norm end\", \"=\"*10)\n",
    "# group norm\n",
    "print(\"=\"*10, \"group norm start\", \"=\"*10)\n",
    "gn = nn.GroupNorm(3, 3)\n",
    "out = gn(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"group norm end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add, sub, mul, div, matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== add start ==========\n",
      "<AddBackward0 object at 0x106de9930>\n",
      "['_saved_alpha']\n",
      "[(<AccumulateGrad object at 0x106dea110>, 0), (<AccumulateGrad object at 0x106dea9e0>, 0)]\n",
      "========== add end ==========\n",
      "========== sub start ==========\n",
      "<SubBackward0 object at 0x106de9930>\n",
      "['_saved_alpha']\n",
      "[(<AccumulateGrad object at 0x106dea1d0>, 0), (<AccumulateGrad object at 0x106de9180>, 0)]\n",
      "========== sub end ==========\n",
      "========== mul start ==========\n",
      "<MulBackward0 object at 0x106d238b0>\n",
      "['_saved_other', '_saved_self']\n",
      "[(<AccumulateGrad object at 0x106de9930>, 0), (<AccumulateGrad object at 0x106dea4d0>, 0)]\n",
      "========== mul end ==========\n",
      "========== div start ==========\n",
      "<DivBackward0 object at 0x106d238b0>\n",
      "['_saved_other', '_saved_self']\n",
      "[(<AccumulateGrad object at 0x106de9930>, 0), (<AccumulateGrad object at 0x106dea4d0>, 0)]\n",
      "========== div end ==========\n",
      "========== matmul start ==========\n",
      "<UnsafeViewBackward0 object at 0x106d238b0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<BmmBackward0 object at 0x106de9930>, 0)]\n",
      "========== matmul end ==========\n",
      "========== bmmbackward start ==========\n",
      "<UnsafeViewBackward0 object at 0x106d238b0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<BmmBackward0 object at 0x106de9930>, 0)]\n",
      "========== bmmbackward end ==========\n",
      "========== mmbackward start ==========\n",
      "<MmBackward0 object at 0x106d238b0>\n",
      "['_saved_mat2', '_saved_mat2_sym_sizes', '_saved_mat2_sym_strides', '_saved_self', '_saved_self_sym_sizes', '_saved_self_sym_strides']\n",
      "[(<AccumulateGrad object at 0x106de9930>, 0), (<AccumulateGrad object at 0x106dea4d0>, 0)]\n",
      "========== mmbackward end ==========\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.randn(1, 3, 4, 4))\n",
    "b = nn.Parameter(torch.randn(1, 3, 4, 4))\n",
    "# add\n",
    "print(\"=\"*10, \"add start\", \"=\"*10)\n",
    "out = a + b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"add end\", \"=\"*10)\n",
    "# sub\n",
    "print(\"=\"*10, \"sub start\", \"=\"*10)\n",
    "out = a - b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"sub end\", \"=\"*10)\n",
    "# mul\n",
    "print(\"=\"*10, \"mul start\", \"=\"*10)\n",
    "out = a * b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"mul end\", \"=\"*10)\n",
    "# div\n",
    "print(\"=\"*10, \"div start\", \"=\"*10)\n",
    "out = a / b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"div end\", \"=\"*10)\n",
    "# matmul\n",
    "print(\"=\"*10, \"matmul start\", \"=\"*10)\n",
    "out = torch.matmul(a, b)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"matmul end\", \"=\"*10)\n",
    "# bmmbackward\n",
    "print(\"=\"*10, \"bmmbackward start\", \"=\"*10)\n",
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "b = torch.randn(1, 3, 4, 5, requires_grad=True)\n",
    "out = a.matmul(b)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"bmmbackward end\", \"=\"*10)\n",
    "# mmbackward\n",
    "print(\"=\"*10, \"mmbackward start\", \"=\"*10)\n",
    "a = torch.randn(4, 4, requires_grad=True)\n",
    "b = torch.randn(4, 5, requires_grad=True)\n",
    "out = a.matmul(b)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"mmbackward end\", \"=\"*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== concat start ==========\n",
      "<CatBackward0 object at 0x106d23d90>\n",
      "['_saved_dim']\n",
      "[(<AccumulateGrad object at 0x106d238b0>, 0), (<AccumulateGrad object at 0x106de9780>, 0)]\n",
      "========== concat end ==========\n",
      "========== split start ==========\n",
      "<SplitBackward0 object at 0x106d23760>\n",
      "['_saved_dim', '_saved_self_sym_sizes', '_saved_split_size']\n",
      "[(<AccumulateGrad object at 0x106d23550>, 0)]\n",
      "========== split end ==========\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.randn(1, 4, 4, 4), requires_grad=True)\n",
    "b = nn.Parameter(torch.randn(1, 6, 4, 4), requires_grad=True)\n",
    "# concat\n",
    "print(\"=\"*10, \"concat start\", \"=\"*10)\n",
    "out = torch.cat([a, b], dim=1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"concat end\", \"=\"*10)\n",
    "# split\n",
    "print(\"=\"*10, \"split start\", \"=\"*10)\n",
    "# out = torch.split(a, 2, dim=1)\n",
    "out = torch.chunk(a, 2, dim=1)\n",
    "out = out[0]\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"split end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatten, reshape, view, unsafeview, clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== flatten start ==========\n",
      "<ReshapeAliasBackward0 object at 0x106de9bd0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x106de9720>, 0)]\n",
      "========== flatten end ==========\n",
      "========== reshape start ==========\n",
      "<ReshapeAliasBackward0 object at 0x106de9bd0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x106de9720>, 0)]\n",
      "========== reshape end ==========\n",
      "========== view start ==========\n",
      "<ViewBackward0 object at 0x106de9bd0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x106de9720>, 0)]\n",
      "========== view end ==========\n",
      "========== unsafeview start ==========\n",
      "<ViewBackward0 object at 0x106de9bd0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x106de9720>, 0)]\n",
      "========== unsafeview end ==========\n",
      "========== clone start ==========\n",
      "<CloneBackward0 object at 0x1419575b0>\n",
      "[]\n",
      "[(<AccumulateGrad object at 0x106de9bd0>, 0)]\n",
      "========== clone end ==========\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "# flatten\n",
    "print(\"=\"*10, \"flatten start\", \"=\"*10)\n",
    "flat = nn.Flatten()\n",
    "out = flat(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"flatten end\", \"=\"*10)\n",
    "# reshape\n",
    "print(\"=\"*10, \"reshape start\", \"=\"*10)\n",
    "out = a.reshape(1, 3, 16)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"reshape end\", \"=\"*10)\n",
    "# view\n",
    "print(\"=\"*10, \"view start\", \"=\"*10)\n",
    "out = a.view(1, 3, 16)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"view end\", \"=\"*10)\n",
    "# unsafeview\n",
    "print(\"=\"*10, \"unsafeview start\", \"=\"*10)\n",
    "out = a.view(1, -1, 16)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"unsafeview end\", \"=\"*10)\n",
    "# clone \n",
    "print(\"=\"*10, \"clone start\", \"=\"*10)\n",
    "out = a.clone()\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"clone end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permute, expansion, squeeze, unsqueeze, transpose, einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== permute start ==========\n",
      "<PermuteBackward0 object at 0x141ddff10>\n",
      "['_saved_dims']\n",
      "[(<AccumulateGrad object at 0x141ddfe80>, 0)]\n",
      "========== permute end ==========\n",
      "========== squeeze start ==========\n",
      "<SqueezeBackward0 object at 0x141ddff10>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x141ddffd0>, 0)]\n",
      "========== squeeze end ==========\n",
      "========== unsqueeze start ==========\n",
      "<UnsqueezeBackward0 object at 0x141ddff10>\n",
      "['_saved_dim']\n",
      "[(<AccumulateGrad object at 0x141ddfeb0>, 0)]\n",
      "========== unsqueeze end ==========\n",
      "========== transpose start ==========\n",
      "<TransposeBackward0 object at 0x106d23d90>\n",
      "['_saved_dim0', '_saved_dim1']\n",
      "[(<AccumulateGrad object at 0x141ddff10>, 0)]\n",
      "========== transpose end ==========\n",
      "========== einops start ==========\n",
      "<ReshapeAliasBackward0 object at 0x106d23d90>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<PermuteBackward0 object at 0x141dc31f0>, 0)]\n",
      "========== einops end ==========\n",
      "========== slice backward single start ==========\n",
      "<SliceBackward0 object at 0x141dc6bc0>\n",
      "['_saved_dim', '_saved_end', '_saved_self_sym_sizes', '_saved_start', '_saved_step']\n",
      "[(<SliceBackward0 object at 0x141dc31f0>, 0)]\n",
      "========== slice backward single end ==========\n",
      "========== expansion start ==========\n",
      "<ExpandBackward0 object at 0x106d23d90>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x141dc31f0>, 0)]\n",
      "========== expansion end ==========\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 3, 4, requires_grad=True)\n",
    "# permute\n",
    "print(\"=\"*10, \"permute start\", \"=\"*10)\n",
    "out = a.permute(0, 2, 1, 3)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"permute end\", \"=\"*10)\n",
    "# squeeze\n",
    "print(\"=\"*10, \"squeeze start\", \"=\"*10)\n",
    "out = a.squeeze()\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"squeeze end\", \"=\"*10)\n",
    "# unsqueeze\n",
    "print(\"=\"*10, \"unsqueeze start\", \"=\"*10)\n",
    "out = a.unsqueeze(0)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"unsqueeze end\", \"=\"*10)\n",
    "# transpose\n",
    "print(\"=\"*10, \"transpose start\", \"=\"*10)\n",
    "out = a.transpose(0, 1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"transpose end\", \"=\"*10)\n",
    "# einops b c h w -> b c (h w)\n",
    "print(\"=\"*10, \"einops start\", \"=\"*10)\n",
    "from einops import rearrange, reduce, repeat\n",
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "out = rearrange(a, 'b c h w -> b c (h w)')\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"einops end\", \"=\"*10)\n",
    "# slice backward single\n",
    "print(\"=\"*10, \"slice backward single start\", \"=\"*10)\n",
    "a = torch.randn(1, 3, 10, 32, requires_grad=True)\n",
    "out = a[:, :16, :, :]\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"slice backward single end\", \"=\"*10)\n",
    "# expansion\n",
    "a = torch.randn(1, 3, 1, 1, requires_grad=True)\n",
    "print(\"=\"*10, \"expansion start\", \"=\"*10)\n",
    "out = a.expand(1, 3, 4, 4)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"expansion end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ReluBackward0 object at 0x10b51a050>\n",
      "['_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<SiluBackward0 object at 0x10b51a050>\n",
      "['_saved_self']\n",
      "[(<AccumulateGrad object at 0x109cc7f40>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<GeluBackward0 object at 0x10b51a050>\n",
      "['_saved_approximate', '_saved_self']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "none\n",
      "====================\n",
      "<HardswishBackward0 object at 0x10b70b340>\n",
      "['_saved_self']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<SigmoidBackward0 object at 0x10b70b340>\n",
      "['_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<TanhBackward0 object at 0x10b70b340>\n",
      "['_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<SoftmaxBackward0 object at 0x10b70b340>\n",
      "['_saved_dim', '_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "1\n",
      "====================\n",
      "<LogSoftmaxBackward0 object at 0x10b51a050>\n",
      "['_saved_dim', '_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "1\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "# relu\n",
    "print(\"=\"*10, \"relu start\", \"=\"*10)\n",
    "out = torch.relu(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"relu end\", \"=\"*10)\n",
    "# silu\n",
    "print(\"=\"*10, \"silu start\", \"=\"*10)\n",
    "out = torch.nn.functional.silu(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"silu end\", \"=\"*10)\n",
    "# gelu\n",
    "print(\"=\"*10, \"gelu start\", \"=\"*10)\n",
    "out = torch.nn.functional.gelu(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"gelu end\", \"=\"*10)\n",
    "# hardswish\n",
    "print(\"=\"*10, \"hardswish start\", \"=\"*10)\n",
    "out = torch.nn.functional.hardswish(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"hardswish end\", \"=\"*10)\n",
    "# sigmoid\n",
    "print(\"=\"*10, \"sigmoid start\", \"=\"*10)\n",
    "out = torch.sigmoid(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"sigmoid end\", \"=\"*10)\n",
    "# tanh\n",
    "print(\"=\"*10, \"tanh start\", \"=\"*10)\n",
    "out = torch.tanh(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"tanh end\", \"=\"*10)\n",
    "# softmax\n",
    "print(\"=\"*10, \"softmax start\", \"=\"*10)\n",
    "out = torch.softmax(a, dim=1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"softmax end\", \"=\"*10)\n",
    "# logsoftmax\n",
    "print(\"=\"*10, \"logsoftmax start\", \"=\"*10)\n",
    "out = torch.log_softmax(a, dim=1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"logsoftmax end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## poolings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1])\n",
      "<MeanBackward1 object at 0x11ba1b9d0>\n",
      "['_saved_dim', '_saved_keepdim', '_saved_self', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x11ba1b700>, 0)]\n",
      "(1, 3, 4, 4)\n",
      "True\n",
      "torch.Size([1, 3, 2, 2])\n",
      "<MaxPool2DWithIndicesBackward0 object at 0x11ba1b9d0>\n",
      "['_saved_ceil_mode', '_saved_dilation', '_saved_kernel_size', '_saved_padding', '_saved_result1', '_saved_self', '_saved_stride']\n",
      "[(<AccumulateGrad object at 0x11ba1ba60>, 0)]\n",
      "torch.Size([1, 3, 4, 4])\n",
      "torch.Size([1, 3, 2, 2])\n",
      "<AvgPool2DBackward0 object at 0x11ba1b700>\n",
      "['_saved_ceil_mode', '_saved_count_include_pad', '_saved_divisor_override', '_saved_kernel_size', '_saved_padding', '_saved_self', '_saved_stride']\n",
      "[(<AccumulateGrad object at 0x11ba1ba60>, 0)]\n",
      "torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "# AdaptiveAvgPool\n",
    "print(\"=\"*10, \"AdaptiveAvgPool start\", \"=\"*10)\n",
    "out = torch.nn.AdaptiveAvgPool2d((1, 1))(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"AdaptiveAvgPool end\", \"=\"*10)\n",
    "# MaxPool\n",
    "print(\"=\"*10, \"MaxPool start\", \"=\"*10)\n",
    "out = torch.nn.MaxPool2d((2, 2))(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"MaxPool end\", \"=\"*10)\n",
    "# AvgPool\n",
    "print(\"=\"*10, \"AvgPool start\", \"=\"*10)\n",
    "out = torch.nn.AvgPool2d((2, 2))(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"AvgPool end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== upsample bilinear ====================\n",
      "torch.Size([1, 3, 64, 64])\n",
      "<UpsampleBilinear2DBackward0 object at 0x280bdabf0>\n",
      "['_saved_align_corners', '_saved_output_size', '_saved_scales_h', '_saved_scales_w', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x281007460>, 0)]\n",
      "==================== upsample nearest ====================\n",
      "torch.Size([1, 3, 64, 64])\n",
      "<UpsampleNearest2DBackward0 object at 0x280bdabf0>\n",
      "['_saved_output_size', '_saved_scales_h', '_saved_scales_w', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x11ba18ca0>, 0)]\n",
      "==================== upsample bicubic ====================\n",
      "torch.Size([1, 3, 64, 64])\n",
      "<UpsampleBicubic2DBackward0 object at 0x11ba18be0>\n",
      "['_saved_align_corners', '_saved_output_size', '_saved_scales_h', '_saved_scales_w', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x11ba1b760>, 0)]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 32, 32, requires_grad=True)\n",
    "# upsample bilinear\n",
    "print(\"=\"*10, \"upsample bilinear start\", \"=\"*10)\n",
    "out = torch.nn.functional.interpolate(a, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"upsample bilinear end\", \"=\"*10)\n",
    "# upsample nearest\n",
    "print(\"=\"*10, \"upsample nearest start\", \"=\"*10)\n",
    "out = torch.nn.functional.interpolate(a, scale_factor=2, mode='nearest')\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"upsample nearest end\", \"=\"*10)\n",
    "# upsample bicubic\n",
    "print(\"=\"*10, \"upsample bicubic start\", \"=\"*10)\n",
    "out = torch.nn.functional.interpolate(a, scale_factor=2, mode='bicubic', align_corners=True)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"upsample bicubic end\", \"=\"*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
