{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial of getting information from backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define some functions\n",
    "some backward has attributes `_saved*`, which save the most important information of calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def print_grad_fn(out):\n",
    "    grad = out.grad_fn\n",
    "    print(grad)\n",
    "    print([attr for attr in dir(grad) if attr[:6]==\"_saved\"])\n",
    "    print([sub_g for sub_g in grad.next_functions])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== convolution 1d start ==========\n",
      "<ConvolutionBackward0 object at 0x177a9b460>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x177a9b370>, 0), (<AccumulateGrad object at 0x177a9b340>, 0)]\n",
      "========== convolution 1d end ==========\n",
      "========== convolution 2d start ==========\n",
      "<ConvolutionBackward0 object at 0x177a9b400>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x177a9b370>, 0), (<AccumulateGrad object at 0x177a9b340>, 0)]\n",
      "========== convolution 2d end ==========\n",
      "========== convolution 3d start ==========\n",
      "<ConvolutionBackward0 object at 0x177a9b460>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x177a9b370>, 0), (<AccumulateGrad object at 0x177a9b340>, 0)]\n",
      "========== convolution 3d end ==========\n",
      "<ConvolutionBackward0 object at 0x177a9b460>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x177a9b370>, 0), (<AccumulateGrad object at 0x177a9b340>, 0)]\n",
      "========== convolution with groups end ==========\n",
      "========== convolution with dilation start ==========\n",
      "<ConvolutionBackward0 object at 0x177a9b400>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x177a9b370>, 0), (<AccumulateGrad object at 0x177a9b340>, 0)]\n",
      "========== convolution with dilation end ==========\n"
     ]
    }
   ],
   "source": [
    "# convlution 1d\n",
    "print(\"=\"*10, \"convolution 1d start\", \"=\"*10)\n",
    "conv = nn.Conv1d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution 1d end\", \"=\"*10)\n",
    "# convolution 2d\n",
    "print(\"=\"*10, \"convolution 2d start\", \"=\"*10)\n",
    "conv = nn.Conv2d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution 2d end\", \"=\"*10)\n",
    "# convolution 3d\n",
    "print(\"=\"*10, \"convolution 3d start\", \"=\"*10)\n",
    "conv = nn.Conv3d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution 3d end\", \"=\"*10)\n",
    "# convolution with groups\n",
    "conv = nn.Conv2d(3, 3, 3, 1, 1, groups=3)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution with groups end\", \"=\"*10)\n",
    "# convolution with dilation\n",
    "print(\"=\"*10, \"convolution with dilation start\", \"=\"*10)\n",
    "conv = nn.Conv2d(3, 4, 3, 1, 1, dilation=2)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"convolution with dilation end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== transpose convolution 2d start ==========\n",
      "<ConvolutionBackward0 object at 0x177a6ae30>\n",
      "['_saved_bias_sym_sizes_opt', '_saved_dilation', '_saved_groups', '_saved_input', '_saved_output_padding', '_saved_padding', '_saved_stride', '_saved_transposed', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x177a69cf0>, 0), (<AccumulateGrad object at 0x177a6ada0>, 0)]\n",
      "True\n",
      "========== transpose convolution 2d end ==========\n"
     ]
    }
   ],
   "source": [
    "# transpose convolution 2d\n",
    "print(\"=\"*10, \"transpose convolution 2d start\", \"=\"*10)\n",
    "conv = nn.ConvTranspose2d(3, 4, 3, 1, 1)\n",
    "out = conv(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(out.grad_fn._saved_transposed)\n",
    "print(\"=\"*10, \"transpose convolution 2d end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Linear start ==========\n",
      "<AddmmBackward0 object at 0x106d20be0>\n",
      "['_saved_alpha', '_saved_beta', '_saved_mat1', '_saved_mat1_sym_sizes', '_saved_mat1_sym_strides', '_saved_mat2', '_saved_mat2_sym_sizes', '_saved_mat2_sym_strides']\n",
      "[(<AccumulateGrad object at 0x141cc5f90>, 0), (<AccumulateGrad object at 0x141cc5ff0>, 0), (<TBackward0 object at 0x141cc5990>, 0)]\n",
      "========== Linear end ==========\n",
      "========== Linear with input more than 2 dims start ==========\n",
      "<AddBackward0 object at 0x106d20be0>\n",
      "['_saved_alpha']\n",
      "[(<UnsafeViewBackward0 object at 0x141cc5f90>, 0), (<AccumulateGrad object at 0x141cc5ff0>, 0)]\n",
      "========== Linear with input more than 2 dims end ==========\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "print(\"=\"*10, \"Linear start\", \"=\"*10)\n",
    "fc = nn.Linear(3, 4)\n",
    "out = fc(torch.randn(1, 3, requires_grad=True))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"Linear end\", \"=\"*10)\n",
    "# Linear with input more than 2 dims\n",
    "print(\"=\"*10, \"Linear with input more than 2 dims start\", \"=\"*10)\n",
    "out = fc(torch.randn(1, 4, 4, 3, requires_grad=True))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"Linear with input more than 2 dims end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== batch norm start ==========\n",
      "<NativeBatchNormBackward0 object at 0x106d23700>\n",
      "['_saved_eps', '_saved_input', '_saved_result1', '_saved_result2', '_saved_running_mean', '_saved_running_var', '_saved_training', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d238b0>, 0), (<AccumulateGrad object at 0x106d23df0>, 0)]\n",
      "========== batch norm end ==========\n",
      "========== layer norm start ==========\n",
      "<NativeLayerNormBackward0 object at 0x106d23550>\n",
      "['_saved_bias', '_saved_input', '_saved_normalized_shape', '_saved_result1', '_saved_result2', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d238b0>, 0), (<AccumulateGrad object at 0x106d23df0>, 0)]\n",
      "========== layer norm end ==========\n",
      "========== group norm start ==========\n",
      "<NativeGroupNormBackward0 object at 0x106d23700>\n",
      "['_saved_C', '_saved_HxW', '_saved_N', '_saved_eps', '_saved_group', '_saved_input', '_saved_result1', '_saved_result2', '_saved_weight']\n",
      "[(None, 0), (<AccumulateGrad object at 0x106d23550>, 0), (<AccumulateGrad object at 0x106d23df0>, 0)]\n",
      "========== group norm end ==========\n"
     ]
    }
   ],
   "source": [
    "# batch norm\n",
    "print(\"=\"*10, \"batch norm start\", \"=\"*10)\n",
    "bn = nn.BatchNorm2d(3)\n",
    "out = bn(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"batch norm end\", \"=\"*10)\n",
    "# # layer norm\n",
    "print(\"=\"*10, \"layer norm start\", \"=\"*10)\n",
    "ln = nn.LayerNorm([3, 4, 4])\n",
    "out = ln(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"layer norm end\", \"=\"*10)\n",
    "# group norm\n",
    "print(\"=\"*10, \"group norm start\", \"=\"*10)\n",
    "gn = nn.GroupNorm(3, 3)\n",
    "out = gn(torch.randn(1, 3, 4, 4))\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"group norm end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add, sub, mul, div, matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== add start ==========\n",
      "<AddBackward0 object at 0x106de9930>\n",
      "['_saved_alpha']\n",
      "[(<AccumulateGrad object at 0x106dea110>, 0), (<AccumulateGrad object at 0x106dea9e0>, 0)]\n",
      "========== add end ==========\n",
      "========== sub start ==========\n",
      "<SubBackward0 object at 0x106de9930>\n",
      "['_saved_alpha']\n",
      "[(<AccumulateGrad object at 0x106dea1d0>, 0), (<AccumulateGrad object at 0x106de9180>, 0)]\n",
      "========== sub end ==========\n",
      "========== mul start ==========\n",
      "<MulBackward0 object at 0x106d238b0>\n",
      "['_saved_other', '_saved_self']\n",
      "[(<AccumulateGrad object at 0x106de9930>, 0), (<AccumulateGrad object at 0x106dea4d0>, 0)]\n",
      "========== mul end ==========\n",
      "========== div start ==========\n",
      "<DivBackward0 object at 0x106d238b0>\n",
      "['_saved_other', '_saved_self']\n",
      "[(<AccumulateGrad object at 0x106de9930>, 0), (<AccumulateGrad object at 0x106dea4d0>, 0)]\n",
      "========== div end ==========\n",
      "========== matmul start ==========\n",
      "<UnsafeViewBackward0 object at 0x106d238b0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<BmmBackward0 object at 0x106de9930>, 0)]\n",
      "========== matmul end ==========\n",
      "========== bmmbackward start ==========\n",
      "<UnsafeViewBackward0 object at 0x106d238b0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<BmmBackward0 object at 0x106de9930>, 0)]\n",
      "========== bmmbackward end ==========\n",
      "========== mmbackward start ==========\n",
      "<MmBackward0 object at 0x106d238b0>\n",
      "['_saved_mat2', '_saved_mat2_sym_sizes', '_saved_mat2_sym_strides', '_saved_self', '_saved_self_sym_sizes', '_saved_self_sym_strides']\n",
      "[(<AccumulateGrad object at 0x106de9930>, 0), (<AccumulateGrad object at 0x106dea4d0>, 0)]\n",
      "========== mmbackward end ==========\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.randn(1, 3, 4, 4))\n",
    "b = nn.Parameter(torch.randn(1, 3, 4, 4))\n",
    "# add\n",
    "print(\"=\"*10, \"add start\", \"=\"*10)\n",
    "out = a + b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"add end\", \"=\"*10)\n",
    "# sub\n",
    "print(\"=\"*10, \"sub start\", \"=\"*10)\n",
    "out = a - b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"sub end\", \"=\"*10)\n",
    "# mul\n",
    "print(\"=\"*10, \"mul start\", \"=\"*10)\n",
    "out = a * b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"mul end\", \"=\"*10)\n",
    "# div\n",
    "print(\"=\"*10, \"div start\", \"=\"*10)\n",
    "out = a / b\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"div end\", \"=\"*10)\n",
    "# matmul\n",
    "print(\"=\"*10, \"matmul start\", \"=\"*10)\n",
    "out = torch.matmul(a, b)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"matmul end\", \"=\"*10)\n",
    "# bmmbackward\n",
    "print(\"=\"*10, \"bmmbackward start\", \"=\"*10)\n",
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "b = torch.randn(1, 3, 4, 5, requires_grad=True)\n",
    "out = a.matmul(b)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"bmmbackward end\", \"=\"*10)\n",
    "# mmbackward\n",
    "print(\"=\"*10, \"mmbackward start\", \"=\"*10)\n",
    "a = torch.randn(4, 4, requires_grad=True)\n",
    "b = torch.randn(4, 5, requires_grad=True)\n",
    "out = a.matmul(b)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"mmbackward end\", \"=\"*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== concat start ==========\n",
      "<CatBackward0 object at 0x136a04d60>\n",
      "['_saved_dim']\n",
      "[(<AccumulateGrad object at 0x145f23250>, 0), (<AccumulateGrad object at 0x145f23220>, 0)]\n",
      "========== concat end ==========\n",
      "========== split start ==========\n",
      "<SplitBackward0 object at 0x145f21e70>\n",
      "['_saved_dim', '_saved_self_sym_sizes', '_saved_split_size']\n",
      "[(<AccumulateGrad object at 0x145f21ed0>, 0)]\n",
      "========== split end ==========\n",
      "========== unsafesplit start ==========\n",
      "<SplitBackward0 object at 0x13689a6b0>\n",
      "['_saved_dim', '_saved_self_sym_sizes', '_saved_split_size']\n",
      "[(<AccumulateGrad object at 0x145f21e70>, 0)]\n",
      "========== unsafesplit end ==========\n",
      "========== unbindbackward start ==========\n",
      "<UnbindBackward0 object at 0x13689a6b0>\n",
      "['_saved_dim']\n",
      "[(<AccumulateGrad object at 0x145f21e70>, 0)]\n",
      "========== unbindbackward end ==========\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.randn(1, 4, 4, 4), requires_grad=True)\n",
    "b = nn.Parameter(torch.randn(1, 6, 4, 4), requires_grad=True)\n",
    "# concat\n",
    "print(\"=\"*10, \"concat start\", \"=\"*10)\n",
    "out = torch.cat([a, b], dim=1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"concat end\", \"=\"*10)\n",
    "# split\n",
    "print(\"=\"*10, \"split start\", \"=\"*10)\n",
    "# out = torch.split(a, 2, dim=1)\n",
    "out = torch.chunk(a, 2, dim=1)\n",
    "out = out[0]\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"split end\", \"=\"*10)\n",
    "# unsafesplit\n",
    "print(\"=\"*10, \"unsafesplit start\", \"=\"*10)\n",
    "out = torch.split(a, 3, dim=0)\n",
    "out = out[0]\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"unsafesplit end\", \"=\"*10)\n",
    "# unbindbackward\n",
    "print(\"=\"*10, \"unbindbackward start\", \"=\"*10)\n",
    "for i in a:\n",
    "    out = i\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"unbindbackward end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatten, reshape, view, unsafeview, clone, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== flatten start ==========\n",
      "<ReshapeAliasBackward0 object at 0x13a9fd450>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x13a9ff340>, 0)]\n",
      "========== flatten end ==========\n",
      "========== reshape start ==========\n",
      "<ReshapeAliasBackward0 object at 0x13a9fd450>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x13a9ff340>, 0)]\n",
      "========== reshape end ==========\n",
      "========== view start ==========\n",
      "<ViewBackward0 object at 0x13a9fd450>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x13a9ff340>, 0)]\n",
      "========== view end ==========\n",
      "========== unsafeview start ==========\n",
      "<ViewBackward0 object at 0x13a9fd450>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x13a9ff340>, 0)]\n",
      "========== unsafeview end ==========\n",
      "========== clone start ==========\n",
      "<CloneBackward0 object at 0x13a9fd450>\n",
      "[]\n",
      "[(<AccumulateGrad object at 0x13a9ff340>, 0)]\n",
      "========== clone end ==========\n",
      "========== repeat start ==========\n",
      "<RepeatBackward0 object at 0x13a9fd450>\n",
      "['_saved_repeats', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x13a9ffc70>, 0)]\n",
      "(1, 3, 1, 1) (1, 3, 4, 4)\n",
      "========== repeat end ==========\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "# flatten\n",
    "print(\"=\"*10, \"flatten start\", \"=\"*10)\n",
    "flat = nn.Flatten()\n",
    "out = flat(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"flatten end\", \"=\"*10)\n",
    "# reshape\n",
    "print(\"=\"*10, \"reshape start\", \"=\"*10)\n",
    "out = a.reshape(1, 3, 16)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"reshape end\", \"=\"*10)\n",
    "# view\n",
    "print(\"=\"*10, \"view start\", \"=\"*10)\n",
    "out = a.view(1, 3, 16)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"view end\", \"=\"*10)\n",
    "# unsafeview\n",
    "print(\"=\"*10, \"unsafeview start\", \"=\"*10)\n",
    "out = a.view(1, -1, 16)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"unsafeview end\", \"=\"*10)\n",
    "# clone \n",
    "print(\"=\"*10, \"clone start\", \"=\"*10)\n",
    "out = a.clone()\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"clone end\", \"=\"*10)\n",
    "# repeat\n",
    "print(\"=\"*10, \"repeat start\", \"=\"*10)\n",
    "out = a.repeat(1, 3, 1, 1)\n",
    "print_grad_fn(out)\n",
    "print(out.grad_fn._saved_repeats, out.grad_fn._saved_self_sym_sizes)\n",
    "print(\"=\"*10, \"repeat end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permute, expansion, squeeze, unsqueeze, transpose, einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== permute start ==========\n",
      "<PermuteBackward0 object at 0x13686b970>\n",
      "['_saved_dims']\n",
      "[(<AccumulateGrad object at 0x145f2f970>, 0)]\n",
      "========== permute end ==========\n",
      "========== squeeze start ==========\n",
      "<SqueezeBackward0 object at 0x13686b970>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x145f2f9a0>, 0)]\n",
      "========== squeeze end ==========\n",
      "========== unsqueeze start ==========\n",
      "<UnsqueezeBackward0 object at 0x13686b970>\n",
      "['_saved_dim']\n",
      "[(<AccumulateGrad object at 0x145f2f9a0>, 0)]\n",
      "========== unsqueeze end ==========\n",
      "========== transpose start ==========\n",
      "<TransposeBackward0 object at 0x13686b970>\n",
      "['_saved_dim0', '_saved_dim1']\n",
      "[(<AccumulateGrad object at 0x145f2f9a0>, 0)]\n",
      "========== transpose end ==========\n",
      "========== einops start ==========\n",
      "<ReshapeAliasBackward0 object at 0x13686b970>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<PermuteBackward0 object at 0x145f2f9a0>, 0)]\n",
      "========== einops end ==========\n",
      "========== expansion start ==========\n",
      "<ExpandBackward0 object at 0x10857c8b0>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x145f2f9a0>, 0)]\n",
      "========== expansion end ==========\n",
      "========== slice backward single start ==========\n",
      "<SliceBackward0 object at 0x13686b970>\n",
      "['_saved_dim', '_saved_end', '_saved_self_sym_sizes', '_saved_start', '_saved_step']\n",
      "[(<SliceBackward0 object at 0x145f2f9a0>, 0)]\n",
      "========== slice backward single end ==========\n",
      "========== index backward start ==========\n",
      "<IndexBackward0 object at 0x10857c8b0>\n",
      "['_saved_indices', '_saved_self_sym_sizes']\n",
      "[(<SliceBackward0 object at 0x145f2f9a0>, 0)]\n",
      "(1, 3, 10, 32) (None, None, tensor([1, 2]))\n",
      "['_saved_dim', '_saved_end', '_saved_self_sym_sizes', '_saved_start', '_saved_step']\n",
      "========== index backward end ==========\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 3, 4, requires_grad=True)\n",
    "# permute\n",
    "print(\"=\"*10, \"permute start\", \"=\"*10)\n",
    "out = a.permute(0, 2, 1, 3)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"permute end\", \"=\"*10)\n",
    "# squeeze\n",
    "print(\"=\"*10, \"squeeze start\", \"=\"*10)\n",
    "out = a.squeeze()\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"squeeze end\", \"=\"*10)\n",
    "# unsqueeze\n",
    "print(\"=\"*10, \"unsqueeze start\", \"=\"*10)\n",
    "out = a.unsqueeze(0)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"unsqueeze end\", \"=\"*10)\n",
    "# transpose\n",
    "print(\"=\"*10, \"transpose start\", \"=\"*10)\n",
    "out = a.transpose(0, 1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"transpose end\", \"=\"*10)\n",
    "# einops b c h w -> b c (h w)\n",
    "print(\"=\"*10, \"einops start\", \"=\"*10)\n",
    "from einops import rearrange, reduce, repeat\n",
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "out = rearrange(a, 'b c h w -> b c (h w)')\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"einops end\", \"=\"*10)\n",
    "# expansion\n",
    "a = torch.randn(1, 3, 1, 1, requires_grad=True)\n",
    "print(\"=\"*10, \"expansion start\", \"=\"*10)\n",
    "out = a.expand(1, 3, 4, 4)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"expansion end\", \"=\"*10)\n",
    "# slice backward single\n",
    "print(\"=\"*10, \"slice backward single start\", \"=\"*10)\n",
    "a = torch.randn(1, 3, 10, 32, requires_grad=True)\n",
    "out = a[:, :16, :, :]\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"slice backward single end\", \"=\"*10)\n",
    "# index backward\n",
    "print(\"=\"*10, \"index backward start\", \"=\"*10)\n",
    "a = torch.randn(1, 3, 10, 32, requires_grad=True)\n",
    "out = a[:, :, [1, 2], :]\n",
    "print_grad_fn(out)\n",
    "print(out.grad_fn._saved_self_sym_sizes, out.grad_fn._saved_indices)\n",
    "print([attr for attr in dir(out.grad_fn.next_functions[0][0]) if attr[:6]==\"_saved\"])\n",
    "print(\"=\"*10, \"index backward end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ReluBackward0 object at 0x10b51a050>\n",
      "['_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<SiluBackward0 object at 0x10b51a050>\n",
      "['_saved_self']\n",
      "[(<AccumulateGrad object at 0x109cc7f40>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<GeluBackward0 object at 0x10b51a050>\n",
      "['_saved_approximate', '_saved_self']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "none\n",
      "====================\n",
      "<HardswishBackward0 object at 0x10b70b340>\n",
      "['_saved_self']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<SigmoidBackward0 object at 0x10b70b340>\n",
      "['_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<TanhBackward0 object at 0x10b70b340>\n",
      "['_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "====================\n",
      "<SoftmaxBackward0 object at 0x10b70b340>\n",
      "['_saved_dim', '_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "1\n",
      "====================\n",
      "<LogSoftmaxBackward0 object at 0x10b51a050>\n",
      "['_saved_dim', '_saved_result']\n",
      "[(<AccumulateGrad object at 0x168a0fbe0>, 0)]\n",
      "torch.Size([1, 3, 4, 4]) torch.Size([1, 3, 4, 4])\n",
      "1\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "# relu\n",
    "print(\"=\"*10, \"relu start\", \"=\"*10)\n",
    "out = torch.relu(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"relu end\", \"=\"*10)\n",
    "# silu\n",
    "print(\"=\"*10, \"silu start\", \"=\"*10)\n",
    "out = torch.nn.functional.silu(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"silu end\", \"=\"*10)\n",
    "# gelu\n",
    "print(\"=\"*10, \"gelu start\", \"=\"*10)\n",
    "out = torch.nn.functional.gelu(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"gelu end\", \"=\"*10)\n",
    "# hardswish\n",
    "print(\"=\"*10, \"hardswish start\", \"=\"*10)\n",
    "out = torch.nn.functional.hardswish(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"hardswish end\", \"=\"*10)\n",
    "# sigmoid\n",
    "print(\"=\"*10, \"sigmoid start\", \"=\"*10)\n",
    "out = torch.sigmoid(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"sigmoid end\", \"=\"*10)\n",
    "# tanh\n",
    "print(\"=\"*10, \"tanh start\", \"=\"*10)\n",
    "out = torch.tanh(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"tanh end\", \"=\"*10)\n",
    "# softmax\n",
    "print(\"=\"*10, \"softmax start\", \"=\"*10)\n",
    "out = torch.softmax(a, dim=1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"softmax end\", \"=\"*10)\n",
    "# logsoftmax\n",
    "print(\"=\"*10, \"logsoftmax start\", \"=\"*10)\n",
    "out = torch.log_softmax(a, dim=1)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"logsoftmax end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## poolings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1])\n",
      "<MeanBackward1 object at 0x11ba1b9d0>\n",
      "['_saved_dim', '_saved_keepdim', '_saved_self', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x11ba1b700>, 0)]\n",
      "(1, 3, 4, 4)\n",
      "True\n",
      "torch.Size([1, 3, 2, 2])\n",
      "<MaxPool2DWithIndicesBackward0 object at 0x11ba1b9d0>\n",
      "['_saved_ceil_mode', '_saved_dilation', '_saved_kernel_size', '_saved_padding', '_saved_result1', '_saved_self', '_saved_stride']\n",
      "[(<AccumulateGrad object at 0x11ba1ba60>, 0)]\n",
      "torch.Size([1, 3, 4, 4])\n",
      "torch.Size([1, 3, 2, 2])\n",
      "<AvgPool2DBackward0 object at 0x11ba1b700>\n",
      "['_saved_ceil_mode', '_saved_count_include_pad', '_saved_divisor_override', '_saved_kernel_size', '_saved_padding', '_saved_self', '_saved_stride']\n",
      "[(<AccumulateGrad object at 0x11ba1ba60>, 0)]\n",
      "torch.Size([1, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 4, 4, requires_grad=True)\n",
    "# AdaptiveAvgPool\n",
    "print(\"=\"*10, \"AdaptiveAvgPool start\", \"=\"*10)\n",
    "out = torch.nn.AdaptiveAvgPool2d((1, 1))(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"AdaptiveAvgPool end\", \"=\"*10)\n",
    "# MaxPool\n",
    "print(\"=\"*10, \"MaxPool start\", \"=\"*10)\n",
    "out = torch.nn.MaxPool2d((2, 2))(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"MaxPool end\", \"=\"*10)\n",
    "# AvgPool\n",
    "print(\"=\"*10, \"AvgPool start\", \"=\"*10)\n",
    "out = torch.nn.AvgPool2d((2, 2))(a)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"AvgPool end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== upsample bilinear ====================\n",
      "torch.Size([1, 3, 64, 64])\n",
      "<UpsampleBilinear2DBackward0 object at 0x280bdabf0>\n",
      "['_saved_align_corners', '_saved_output_size', '_saved_scales_h', '_saved_scales_w', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x281007460>, 0)]\n",
      "==================== upsample nearest ====================\n",
      "torch.Size([1, 3, 64, 64])\n",
      "<UpsampleNearest2DBackward0 object at 0x280bdabf0>\n",
      "['_saved_output_size', '_saved_scales_h', '_saved_scales_w', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x11ba18ca0>, 0)]\n",
      "==================== upsample bicubic ====================\n",
      "torch.Size([1, 3, 64, 64])\n",
      "<UpsampleBicubic2DBackward0 object at 0x11ba18be0>\n",
      "['_saved_align_corners', '_saved_output_size', '_saved_scales_h', '_saved_scales_w', '_saved_self_sym_sizes']\n",
      "[(<AccumulateGrad object at 0x11ba1b760>, 0)]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, 3, 32, 32, requires_grad=True)\n",
    "# upsample bilinear\n",
    "print(\"=\"*10, \"upsample bilinear start\", \"=\"*10)\n",
    "out = torch.nn.functional.interpolate(a, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"upsample bilinear end\", \"=\"*10)\n",
    "# upsample nearest\n",
    "print(\"=\"*10, \"upsample nearest start\", \"=\"*10)\n",
    "out = torch.nn.functional.interpolate(a, scale_factor=2, mode='nearest')\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"upsample nearest end\", \"=\"*10)\n",
    "# upsample bicubic\n",
    "print(\"=\"*10, \"upsample bicubic start\", \"=\"*10)\n",
    "out = torch.nn.functional.interpolate(a, scale_factor=2, mode='bicubic', align_corners=True)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"upsample bicubic end\", \"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== MultiheadAttention start ==========\n",
      "<ViewBackward0 object at 0x13a9ff310>\n",
      "['_saved_self_sym_sizes']\n",
      "[(<AddmmBackward0 object at 0x13a8b6d10>, 0)]\n",
      "((<SoftmaxBackward0 object at 0x13a9ff4c0>, 0), (<TransposeBackward0 object at 0x13a8b6740>, 0))\n",
      "========== MultiheadAttention end ==========\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "mha = nn.MultiheadAttention(32, 4)\n",
    "a = torch.randn(1, 32, 32, requires_grad=True)\n",
    "# MultiheadAttention\n",
    "print(\"=\"*10, \"MultiheadAttention start\", \"=\"*10)\n",
    "out = mha(a, a, a)\n",
    "print_grad_fn(out[0])\n",
    "print(out[0].grad_fn.next_functions[0][0].next_functions[1][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(\"=\"*10, \"MultiheadAttention end\", \"=\"*10)\n",
    "\n",
    "from unip.core.pruner import forward_hook\n",
    "hook = mha.register_forward_hook(forward_hook)\n",
    "\n",
    "out = mha(a, a, a)\n",
    "hook.remove()\n",
    "print(list(out[0].grad_fn.next_functions[0][0].metadata.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Embedding start ==========\n",
      "torch.Size([2, 4, 3])\n",
      "<EmbeddingBackward0 object at 0x13b78a1a0>\n",
      "['_saved_indices', '_saved_padding_idx', '_saved_scale_grad_by_freq', '_saved_sparse', '_saved_weight_sym_argsize_0']\n",
      "[(<AccumulateGrad object at 0x13b78a080>, 0)]\n",
      "========== Embedding end ==========\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_fill_padding_idx_with_zero', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'cpu', 'cuda', 'double', 'dump_patches', 'embedding_dim', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'max_norm', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm_type', 'num_embeddings', 'padding_idx', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_parameters', 'scale_grad_by_freq', 'set_extra_state', 'share_memory', 'sparse', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad']\n",
      "====================\n",
      "tensor([[0, 2, 0, 1],\n",
      "        [1, 3, 4, 4]])\n",
      "18446744073709551615\n",
      "False\n",
      "False\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(5, 3)\n",
    "input_tensor = torch.LongTensor([[0, 2, 0, 1], [1, 3, 4, 4]])\n",
    "# Embedding\n",
    "print(\"=\"*10, \"Embedding start\", \"=\"*10)\n",
    "out = embedding(input_tensor)\n",
    "print(out.shape)\n",
    "print_grad_fn(out)\n",
    "print(\"=\"*10, \"Embedding end\", \"=\"*10)\n",
    "\n",
    "print([attr for attr in dir(embedding)])\n",
    "print(\"=\"*20)\n",
    "print(out.grad_fn._saved_indices)\n",
    "print(out.grad_fn._saved_padding_idx)\n",
    "print(out.grad_fn._saved_sparse)\n",
    "print(out.grad_fn._saved_scale_grad_by_freq)\n",
    "print(out.grad_fn._saved_weight_sym_argsize_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
